{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Unveiling the Crystal Ball of Education\"\n",
    "subtitle: Team Java\n",
    "author: Julia Chu, Victoria Shi, Yiru Zhang, and Yuyan Zhang\n",
    "date: 02/27/2023\n",
    "number-sections: true\n",
    "abstract: _Using data from two Portuguese secondary schools, this project examines the connection between individual factors and educational outcomes. Focusing on inference rather than prediction, this study creates three classification models to investigate the explanatory correlation between student demographics, social and school-related characteristics, and the final year grade, G3. The findings indicate that past evaluations have a significant impact on student achievement, and that education attendance plays a significant role in determining educational success regardless of socioeconomic and demographic factors. In addition, the study suggests that policymakers and educators prioritize improving attendance rates and provide targeted support for students who are struggling. Parents can also play an important role in their children's education. The study concludes that additional research is required to identify additional strategies for enhancing academic achievement_.\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116f49b",
   "metadata": {},
   "source": [
    "## Background / Motivation\n",
    "Education is fundamental to one's own growth and that of society in its entirety as it equips individuals with the information, abilities, and morals they'll need to thrive in modern life. Certainly not always, but quite frequently. Gains in income, decreased poverty, and increased productivity all point to it as a major factor in the expansion of the economy and the improvement of social conditions.\n",
    "\n",
    "Our research will look into what role personal attributes play in explaining academic success or failure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff1421",
   "metadata": {},
   "source": [
    "## Problem statement \n",
    "\n",
    "The problem statement involves examining the correlation between individual factors and educational outcomes with the aim of identifying the factors that have the greatest influence on student performance. The project's specific objective is to determine which characteristics are associated with a greater likelihood of passing or failing. This is an inference problem. \n",
    "\n",
    "This is also a classification problem because the final grade is not continuous and will be encoded as two values (pass and fail). To evaluate model accuracy, multiple metrics, including accuracy, precision, and recall, will be considered. Our primary objective is to identify variables that correlate with failure in order to provide additional resources to students who are struggling. Therefore, we will reduce the number of false positives, allowing us to identify more students who require assistance. ROC-AUC is a useful metric because it allows us to specify a false positive/false negative measurement threshold.\n",
    "\n",
    "Priority in model development is variable selection that identifies statistically significant relationships with the final grade . This may also involve employing wrapper methods such as stepwise regression, forward selection, and backward selection to determine the optimal subset of predictors. In addition, lasso and ridge regression may be utilized to prevent multicollinearity predictors and overfitting.\n",
    "\n",
    "The overall project objective is to gain a deeper understanding of the complex factors that impact educational success and student performance. By identifying the variables most strongly associated with passing or failing, we can provide targeted resources and support to students who are struggling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7b95f",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "The information was obtained from [Kaggle](https://www.kaggle.com/datasets/dipam7/student-grade-prediction). In reality, though, the data originates from the UCL Machine Learning Repository and the [University of Minho in Portugal](https://pcortez.dsi.uminho.pt/). The data set contains student demographic and performance statistics from two Portuguese secondary schools. And we intend to identify the specific characteristics (which will serve as our predictors) that influence the final G3 grade for the year (outcome variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c255035",
   "metadata": {},
   "source": [
    "## Stakeholders\n",
    "A wide variety of stakeholders, such as parents, students, professionals working in the education business, economists, and policymakers, could perhaps have an interest in our projects. It is possible that through examining the dataset and gaining insights, we can discover ways in which school curricula can be improved as well as methods by which policymakers and economists can provide better support for future generations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9994e",
   "metadata": {},
   "source": [
    "## Data cleaning and quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f814dd",
   "metadata": {},
   "source": [
    "There are no missing values in the dataset. During the data quality check, some outliers were identified in each predictor variable, but after conducting statistical tests, it was determined that these outliers were not influential points and would not significantly impact the model. However, data preparation and transformation were further conducted to enhance model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddf1c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical columns \n",
      "['age', 'Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G1', 'G2', 'G3']\n",
      "\n",
      "categorical columns: \n",
      "['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>traveltime</th>\n",
       "      <th>studytime</th>\n",
       "      <th>failures</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>395.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.696203</td>\n",
       "      <td>2.749367</td>\n",
       "      <td>2.521519</td>\n",
       "      <td>1.448101</td>\n",
       "      <td>2.035443</td>\n",
       "      <td>0.334177</td>\n",
       "      <td>3.944304</td>\n",
       "      <td>3.235443</td>\n",
       "      <td>3.108861</td>\n",
       "      <td>1.481013</td>\n",
       "      <td>2.291139</td>\n",
       "      <td>3.554430</td>\n",
       "      <td>5.708861</td>\n",
       "      <td>10.908861</td>\n",
       "      <td>10.713924</td>\n",
       "      <td>10.415190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.276043</td>\n",
       "      <td>1.094735</td>\n",
       "      <td>1.088201</td>\n",
       "      <td>0.697505</td>\n",
       "      <td>0.839240</td>\n",
       "      <td>0.743651</td>\n",
       "      <td>0.896659</td>\n",
       "      <td>0.998862</td>\n",
       "      <td>1.113278</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>1.287897</td>\n",
       "      <td>1.390303</td>\n",
       "      <td>8.003096</td>\n",
       "      <td>3.319195</td>\n",
       "      <td>3.761505</td>\n",
       "      <td>4.581443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age        Medu        Fedu  traveltime   studytime    failures  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean    16.696203    2.749367    2.521519    1.448101    2.035443    0.334177   \n",
       "std      1.276043    1.094735    1.088201    0.697505    0.839240    0.743651   \n",
       "min     15.000000    0.000000    0.000000    1.000000    1.000000    0.000000   \n",
       "25%     16.000000    2.000000    2.000000    1.000000    1.000000    0.000000   \n",
       "50%     17.000000    3.000000    2.000000    1.000000    2.000000    0.000000   \n",
       "75%     18.000000    4.000000    3.000000    2.000000    2.000000    0.000000   \n",
       "max     22.000000    4.000000    4.000000    4.000000    4.000000    3.000000   \n",
       "\n",
       "           famrel    freetime       goout        Dalc        Walc      health  \\\n",
       "count  395.000000  395.000000  395.000000  395.000000  395.000000  395.000000   \n",
       "mean     3.944304    3.235443    3.108861    1.481013    2.291139    3.554430   \n",
       "std      0.896659    0.998862    1.113278    0.890741    1.287897    1.390303   \n",
       "min      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "25%      4.000000    3.000000    2.000000    1.000000    1.000000    3.000000   \n",
       "50%      4.000000    3.000000    3.000000    1.000000    2.000000    4.000000   \n",
       "75%      5.000000    4.000000    4.000000    2.000000    3.000000    5.000000   \n",
       "max      5.000000    5.000000    5.000000    5.000000    5.000000    5.000000   \n",
       "\n",
       "         absences          G1          G2          G3  \n",
       "count  395.000000  395.000000  395.000000  395.000000  \n",
       "mean     5.708861   10.908861   10.713924   10.415190  \n",
       "std      8.003096    3.319195    3.761505    4.581443  \n",
       "min      0.000000    3.000000    0.000000    0.000000  \n",
       "25%      0.000000    8.000000    9.000000    8.000000  \n",
       "50%      4.000000   11.000000   11.000000   11.000000  \n",
       "75%      8.000000   13.000000   13.000000   14.000000  \n",
       "max     75.000000   19.000000   19.000000   20.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column Name</th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Unique Values</th>\n",
       "      <th>Value Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>school</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'GP': 349, 'MS': 46}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sex</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'F': 208, 'M': 187}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>address</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'U': 307, 'R': 88}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>famsize</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'GT3': 281, 'LE3': 114}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pstatus</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'T': 354, 'A': 41}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mjob</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'other': 141, 'services': 103, 'at_home': 59,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Fjob</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'other': 217, 'services': 111, 'teacher': 29,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>reason</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>{'course': 145, 'home': 109, 'reputation': 105...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>guardian</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>{'mother': 273, 'father': 90, 'other': 32}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>schoolsup</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'no': 344, 'yes': 51}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>famsup</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'yes': 242, 'no': 153}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>paid</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'no': 214, 'yes': 181}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>activities</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'yes': 201, 'no': 194}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nursery</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'yes': 314, 'no': 81}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>higher</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'yes': 375, 'no': 20}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>internet</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'yes': 329, 'no': 66}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>romantic</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'no': 263, 'yes': 132}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Column Name Missing Values Unique Values  \\\n",
       "0       school              0             2   \n",
       "1          sex              0             2   \n",
       "2      address              0             2   \n",
       "3      famsize              0             2   \n",
       "4      Pstatus              0             2   \n",
       "5         Mjob              0             5   \n",
       "6         Fjob              0             5   \n",
       "7       reason              0             4   \n",
       "8     guardian              0             3   \n",
       "9    schoolsup              0             2   \n",
       "10      famsup              0             2   \n",
       "11        paid              0             2   \n",
       "12  activities              0             2   \n",
       "13     nursery              0             2   \n",
       "14      higher              0             2   \n",
       "15    internet              0             2   \n",
       "16    romantic              0             2   \n",
       "\n",
       "                                         Value Counts  \n",
       "0                               {'GP': 349, 'MS': 46}  \n",
       "1                                {'F': 208, 'M': 187}  \n",
       "2                                 {'U': 307, 'R': 88}  \n",
       "3                            {'GT3': 281, 'LE3': 114}  \n",
       "4                                 {'T': 354, 'A': 41}  \n",
       "5   {'other': 141, 'services': 103, 'at_home': 59,...  \n",
       "6   {'other': 217, 'services': 111, 'teacher': 29,...  \n",
       "7   {'course': 145, 'home': 109, 'reputation': 105...  \n",
       "8          {'mother': 273, 'father': 90, 'other': 32}  \n",
       "9                              {'no': 344, 'yes': 51}  \n",
       "10                            {'yes': 242, 'no': 153}  \n",
       "11                            {'no': 214, 'yes': 181}  \n",
       "12                            {'yes': 201, 'no': 194}  \n",
       "13                             {'yes': 314, 'no': 81}  \n",
       "14                             {'yes': 375, 'no': 20}  \n",
       "15                             {'yes': 329, 'no': 66}  \n",
       "16                            {'no': 263, 'yes': 132}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_csv(\"data/student-mat.csv\")\n",
    "\n",
    "num_col = list(df.select_dtypes(include=['int64', 'float64']))\n",
    "cat_col = list(df.select_dtypes(include=['object']))\n",
    "print(f\"numerical columns \\n{num_col}\\n\")\n",
    "print(f\"categorical columns: \\n{cat_col}\")\n",
    "\n",
    "#Numerical Predictors Distribution\n",
    "display(df[num_col].describe())\n",
    "\n",
    "cat_table = pd.DataFrame(columns=['Column Name', 'Missing Values', 'Unique Values', 'Value Counts'])\n",
    "\n",
    "for col in cat_col:\n",
    "    missing_values = df[col].isnull().sum()\n",
    "    unique_values = df[col].nunique()\n",
    "    value_counts = df[col].value_counts().to_dict()\n",
    "    cat_table = cat_table.append({'Column Name': col, \n",
    "                                  'Missing Values': missing_values, \n",
    "                                  'Unique Values': unique_values, \n",
    "                                  'Value Counts': value_counts}, ignore_index=True)\n",
    "\n",
    "display(cat_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "## Data preparation and transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f548a002",
   "metadata": {},
   "source": [
    "It is important to note that the data used in this analysis has no missing values. However, this may not reflect the typical cleanliness and completeness of real-world data sets, which often contain missing or inconsistent values and require more extensive cleaning and preparation. In future work, it will be important to take these considerations into account and apply appropriate data cleaning and preparation techniques to ensure the validity of the results.\n",
    "\n",
    "In the data preparation phase, several steps were taken to transform and prepare the data for analysis. The following is a summary of the steps taken:\n",
    "\n",
    "- Identification of categorical variables: All columns in the data frame were reviewed to identify the categorical variables. A loop comprehension was used to find the number of unique values corresponding to each column, and this information was used to decide on the data processing approach.\n",
    "- Conversion of yes-no variables to binary variables: The code created a dictionary for binary mapping and applied it to all columns in the data frame that contained 'yes' or 'no' as the response.\n",
    "- Transformation of predictors with 2 unique values: The code transformed predictors with two unique values into binary variables by mapping them to 0 or 1. This was done for variables such as \"school\", \"sex\", \"famsize\", \"address\", and \"Pstatus\".\n",
    "- Creation of new predictors: To handle variables with more than two unique values, the code created dummy variables using the \"get_dummies\" function in pandas. The dummy variables were created for the \"Mjob\", \"Fjob\", \"reason\", and \"guardian\" columns. The original columns were then dropped, and the dummy variables were concatenated with the original data frame. Additionally, \"G3\" grades were separated into fourth quartiles and turned into dummy variables to be used for the nested model.\n",
    "- Combination of correlated predictors: The code combined the \"Dalc\" and \"Walc\" columns into a single \"Alc\" column to reduce correlation between the two variables, and combined the \"Fedu\" and \"Medu\" columns into a single \"famEdu\" column to capture the combined education of both parents. This helped to reduce data redundancy and the noise in the data set, and removed the multicollinearity.\n",
    "- Conversion of data types: The original data frame, which consisted of both categorical and numerical values, was converted into one that only consisted of numerical data types or uint8. This made it easier and more convenient for later variable selection and model development.\n",
    "\n",
    "In conclusion, the data cleaning and preparation phase transformed and prepared the data for analysis by converting categorical variables into binary or numerical values, combining correlated predictors, and converting the data types into a more convenient format for analysis. These steps helped to ensure the validity of the results and facilitated later variable selection and model development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc597c8",
   "metadata": {},
   "source": [
    "1. During the base model development, we transformed the response variable G3 grade into a binary variable using the common pass/fail boundary as the standard. In an exam worth a total of 20 points, a grade of 12 or higher is considered passing, while anything below is deemed failing. We used this standard to classify if a student passes or fails consistently throughout the analysis. We plotted the grade distribution in its raw format (out of 20) and in its binary form, respectively, to visualize the general distribution of student grades.\n",
    "2. Period 1 (`G1`) and Period 2 (`G2`) grades are strongly correlated with each other, as evidenced by the darker shade on the pairwise correlation plot.\n",
    "3. Period 1 (`G1`) and Period 2 (`G2`) grades are highly indicative of the final grade (`G3`), as evidenced by the scatterplot displaying the relationship between `G1` and `G2`, with data points colored based on `G3`.\n",
    "4. Both the bar plot based on importance score of the decision tree and the line plot demonstrate that `absences` and `failures` are crucial for predicting the final grades, with grades decreasing as absences and failures increase.\n",
    "5. `Medu` (mother's education) and `Fedu` (father's education) are strongly correlated, so we combined `Medu` and `Fedu` into `famEdu` (family education).\n",
    "6. `Dalc` (weekday alcohol consumption) and `Walc` (weekend alcohol consumption) are strongly correlated, so we combined `Dalc` and `Walc` into `Alc` (alcohol consumption).\n",
    "7. A new correlation plot after removing and combining predictors shows that major dependencies among predictors have been resolved.\n",
    "8. Categorical predictors like parents' jobs (`Mjob` and `Fjob`) also affect a student's grades. However, due to the difficulty in generalizing their impact across various categories, we did not focus on this factor during the base model development. Nevertheless, it remains an important aspect to consider in future analyses or more specialized models.\n",
    "\n",
    "For the nested regression model (explained in more detail in the model development section), a heatmap was used to identify the correlation of the predictors with `G3` quartiles as dummy variables. Predictors that seemed like they had a potential to be important were further explored through boxplots. These include `Medu`, `failures`, `goout`, and `Fjob_teacher`.\n",
    "\n",
    "\n",
    "For the model that predicts progress between different tests, the insights used are different form the insights used for the base model because we have created new categorical variables as our responses: `G1_G2` and `G2_G3`, which will be 1 if a student improves from the previous test and 0 if a student gets the same or lower score. I visualize the relationship between the predictors and `G1_G2` and `G2_G3` using barplots because a lot of the predictors are binary and other types of plots make it hard to observe the trend. Here are some findings: \n",
    "\n",
    "1. Observing the barplots of the predictors versus `G1_G2`, I see that the predictors `age`, `reason_other`, `Mjob other`, `romantic`, and `Fjob_services` have some kind of relationship with our response variables: as their values change, the mean of the reponse variable will change relatively more than the plots for other predictors. Note: I did not include all the graphs in the code file because we have 43 predictors in the dataset. \n",
    "\n",
    "2. Similarly, based on the barplots of predictors versus `G2_G3`, the variables `Mjob_teacher`, `famrel`, and `reason_course` seem to have the most effect on the reponse variable as the predictors' values change. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c782c",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa761aba",
   "metadata": {},
   "source": [
    "During our data exploration, we recognized that predicting a continuous score from mostly binary predictor variables would be challenging. This is because binary variables typically have a non-linear relationship with the outcome variable, which can lead to inaccurate predictions and a lack of precision in capturing the nuances of the data. Thus, we decided to approach the research question with a classification method, which allowed us to develop the following three different models for different use cases:\n",
    "\n",
    "- Base model used for inference: This model is designed to determine the predictors that have the most impact on students' final grades.\n",
    "- Nested regression model for prediction: This model identifies students expected to perform well in the class based on their earlier semester performance.\n",
    "- Progress model: This model examines the factors associated with students' improvement between the first and second grading periods.\n",
    "\n",
    "In our classification models, we prioritize minimizing the false positive rate (FPR) as it is crucial to identify and assist students who genuinely require help with their grades. Misclassifying a student who needs help as not needing it may not be as harmful as the opposite scenario, where a student who genuinely needs help is not identified and subsequently fails. Thus, our focus is on reducing FPR to minimize the chances of misclassifying students in need. In the nested model, FNR is minimized for the same reasons but is different due to doing well being coded as a 0, and not well coded to a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a581854",
   "metadata": {},
   "source": [
    "In this study, we utilized a manual categorization approach to convert the continuous response variable into a categorical response. W set a threshold and categorized the values greater than the threshold as \"pass\" and values less than or equal to the threshold as \"fail\". This valid method allowed us to effectively predict student performance through classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49795297",
   "metadata": {},
   "source": [
    "### Base Model Development\n",
    "\n",
    "Several techniques were used to develop and evaluate the model for the initial and variable selection phase and base model development. The following is a summary of the techniques used:\n",
    "\n",
    "- K-Fold cross-validation and train-test split: K-Fold cross-validation and train-test split were used to assess the model's accuracy. In K-Fold cross-validation, the data was divided into K equal parts, and the model was trained and tested K times, with each part being used as the test set once. In the train-test split, the data was divided into a training set and a validation set, and the model was trained on the training set and tested on the validation set.\n",
    "\n",
    "Two different feature selection methods were employed to identify the most important quantitative and categorical predictors.\n",
    "\n",
    "For identifying the most important categorical predictors, the SelectKBest method with the chi-square test was used. First, only the categorical columns were extracted from the dataset. Then, the SelectKBest method was applied to choose the top 5 features based on their chi-square scores. The scores were stored in a pandas Series object and sorted in descending order to display the most significant categorical predictors.\n",
    "\n",
    "- Decision tree search:  To select the most important quantitative predictors, a Decision Tree Regressor model was fitted to the training data. We used `DecisionTreeRegressor` as opposed to `DecisionTreeClassifier` as we were treating the problem as a prediction problem on the first hand, followed by manual classification based on the pass/fail threshold.  Feature importances were calculated, and used a horizontal bar plot was created to visualize the top 10 features with the largest importance values. Variables `failures` and `absences` were identified as the two most important features.\n",
    "\n",
    "- Chi-square test variable selection: The chi-square test was used to select the most important categorical predictors by assessing the dependence between the categorical predictors and the target variable. `schoolsup` (extra educational support( and parents' job (mother's job `Mjob` and father's job `Fjob`) were important as relatively important predictors.\n",
    "\n",
    "Given the limitations of a non-linear regressor (`DecisionTreeRegressor`) in identifying important features for linear regression, more than one decision tree was built to analyze and identify the important features. While the selected features often vary,  the most important factors kept at each tree's top branch - features consistently selected as the most important ones regardless of trees built - were `failures` and `absences`. With this information, the base model was developed using only these 2 most important features, along with their interaction terms.\n",
    "\n",
    "Four models was fitted using all possible combinations of predictors `failures` and `absences`:\n",
    "1. $G3 \\sim absences$\n",
    "2. $G3 \\sim failures$\n",
    "3. $G3 \\sim absences + failures$\n",
    "4. $G3 \\sim absences * failures$\n",
    "\n",
    "All achieved a 100% accuracy on the training and testing data. Accuracy was used as the only metric at this stage, as it provides a direct measure of the model's basic performance without giving weight to the cost of different errors. Later, more refined metrics will be used to weigh the priorities and relative costs of different types of errors.\n",
    "\n",
    "We chose $G3 \\sim absences * failures$ as the final base model as it captures the meaningful relationship of these two features and their combined effect on predicting student grades (`G3`). The term `absences:failures` has a P-value as small as 1.473151e-03. The extremely small P-value means that the interaction is statistically significant.The other models, although achieving an accuracy of 1.0 on the test set and the full dataset, do not account for the interaction between absences and failures. This could lead to a less accurate prediction of student grades in real-world scenarios where the the effect of absences on student grades is not constant across students who have experienced different levels of failures. By including the interaction term, the last model better accounts for the combined influence of absences and failures on student grades, making it the best choice among the four models presented.\n",
    "\n",
    "Final base model: $$G3 \\sim absences * failures$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a55bb1",
   "metadata": {},
   "source": [
    "### Nested Model Development\n",
    "The nested model was developed through manual trial and error, by adding predictor identified as possibly important during EDA to see if it improves prediction accuracy.\n",
    "\n",
    "`logit_model_1` uses `G1` and `G2` as predictors and classifies `G3` grade into the (0) top or (1) bottom 50% of the class. Since `G1` and `G2` are collinear, the model is not useful for inference, but multicollinearity is ignored as it does not effect prediction. A threshold of 0.3 was used by examining the precision-recall curve. In this case, we want to minimize the false negative rate (incorrectly classify students who are actually in the bottom 50% as being in the top 50%), so we want to have a high recall. Compared to a threshold of 0.5, a threshold of 0.3 both decreased FNR while increasing accuracy on both train and test data.\n",
    "\n",
    "Using the predicted values of `logit_model_1` the data is split into two dataframes one containing all the predicted top 50% observations, and one containing the predicted bottom 50%. To create `logit_model_2`, which classifies whether an observation falls in the first or second quartile, additional predictors identified during EDA were added to the model. However, all except `G1` and `G2` were removed from due to high coefficient p-values, meaning that the predictors were likely insignificant. It was more difficult to choose a classification threshold probability for this model, as it's even more important to minimize false negatives (students who are in the lower 25% of the class and really struggling incorrectly classified as doing okay). However, there is a big trade-off between precision and recall. To get a false negative rate of under 10% for both test and train data, accuracy rate drops to below 90%. At the chosen threshold of 0.35, train FNR is 9.5% and accuracy is 87.1%, and test FNR is 5.6% and accuracy is 88.9%.\n",
    "\n",
    "For the modeling predicting whether the upper 50% subset falls into the third of fourth quantile (`logit_model_3`), a similar approach to `logit_model_2` was taken. For that model, the most significant predictor is `G2`. With that predictor alone was able to classify train data with 93.8% accuracy and test data with 100% accuracy. Adding other predictors did not increase the accuracy of the model for either the training or test data. Threshold was kept at 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db21c478",
   "metadata": {},
   "source": [
    "### Progress Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42722f3",
   "metadata": {},
   "source": [
    "The progress model seeks to identify attributes that indicate whether a student has improved between G1 and G2 and between G2 and G3.\n",
    "\n",
    "We considered building on top of previous findings. However, using the same set of predictor from previous models does not yield convincing results:\n",
    "\n",
    "1. Models are not significant: best LLR p-value achieved for the model for G1_G2 is around 0.01, which is far less than other models built in this project. \n",
    "2. Most of variables are insignificant as the p-value for individual coefficient is high.\n",
    "3. Measurements of classification accuracy is largely affected by the chosen threshold: even if we change the threshold by 0.01, there might be huge change in the observations that belong to FN and FP, so we need to balance between accuracy and recall.\n",
    "4. After we balance the measurements and choose a fixed threshold, the highest accuracy/recall is less than 65% and the lowest FPR/FNR is larger than 35%\n",
    "5. We also tried to calculate ROC-AUC, which is independent from the threshold, and the value is only around 0.65. \n",
    "\n",
    "This part of the exploration is not shown in the code file, but based on this attempt, the direction of the prgress model is completely different from the base model: we will do new variable selection and new models.\n",
    "\n",
    "To select the best predictors, Lasso is performed on the train dataset twice, because we need two new models for G1_G2 and G2_G3. We definitely don't want too many predictors in our models because giving too many advices to the students and families might make them feel overwhelmed. Therefore, I selected five predictors based on the absolute values of the coefficients after Lasso. It turns out that they match with our EDA but have small differences. \n",
    "\n",
    "For the model built for G1_G2, there is one selected variable from Lasso that is different from the variables selected in EDA, so I build two models and compare them. Since our main concern is to help students and we don't want to leave out any student who actually needs help. In other words, predicting a student as able to improve when they cannot (False positives) will be harmful because the teacher will put effort in this student. Thus, the most important metric in the classification of improvement is FP. The model based on Lasso has lower FPR and other measurements are reasonable, so we will use progress_model_1_1 to predict whether a student improves from the first test to the second test. \n",
    "\n",
    "We will have similar consideration for the two models builts for G2_G3. However, in this scenario, the model built using the predictors from Lasso has much higher recall (43.8) than model using predictors from EDA (25%) while the FPR of the Lasso model (41.2%) is much higher than the FPR of the model using EDA (28.8%). Since both models have an obvious disadvantage, we will select the final model based on what we concern more--FPR, so we will use progress_model_2_2 to predict whether a student improves from the second test to the final test. \n",
    "\n",
    "Final Progress model: \n",
    "1. $$G1_G2 \\sim 6.3710 - 0.4348* age + 0.9210* reason_other + 0.5279* Mjob_other - 0.4735* roantic - 0.1581* famrel$$\n",
    "2. $$G2_G3 \\sim -2.4021 + 0.6112* Mjob_teacher - 0.4904* reason_course + 0.3824* famrel$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f772f64",
   "metadata": {},
   "source": [
    "Despite the limitations of the current dataset, the following conclusions and recommendations can be beneficial to stakeholders:\n",
    "\n",
    "Findings: \n",
    "\n",
    "1. Absences and failures are important quantitative predictors in determining a student's grades (Base Model).\n",
    "2. School support and parents' jobs have emerged as important quantitative predictors of students' grades in our analysis (Base Model). \n",
    "3. The mother's job is a significant predictor of academic improvement, with mothers working in teaching and other professions showing a positive relationship with the improvement of grades from G1 to G2 and G2 to G3. On the other hand, mothers working in health care and civil service may have limited time and resources to support their children's education, while housewives may lack the necessary knowledge to help their kids study, both leading to a stagnant predictor in students' academic improvement (Progress Model).\n",
    "4. Students' reasons for choosing a particular school are related to their academic progress. The variable used to measure this consists of values such as proximity to home, school reputation, course preference, and others. Interestingly, the \"other\" category showed a positive coefficient in the model from G1 to G2. Conversely, the \"course\" category had a negative coefficient in the model for G2 to G3, suggesting that students who chose the school based solely on their interest in the courses were less likely to care about their grades. Due to data limitation, we were unable to determine what motivational factors play into the 'other' category. Potentail reasons may include personal growth, knowledge acquisition, or other contextual factors.\n",
    "\n",
    "Recommendation: \n",
    "\n",
    "1. Develop and implement an early warning system that takes into account students' attendance and performance as well as the influence of school support and parents' jobs on academic success. This system should allow for close monitoring of students who may be at risk and provide tailored interventions to address their specific needs, ultimately enhancing their chances of success.\n",
    "2. Encourage parental involvement in their child's education. Provide targeted support and resources to students whose mothers work in health care and civil service (or similar fields) and those whose mothers are housewives. This support could include after-school tutoring programs, access to educational resources, and involvement in school-based activities to enhance parents' understanding of how to support their children's education.\n",
    "3. Schools should prioritize personal growth and knowledge acquisition opportunities, in addition to offering interesting courses. This can include non-academic challenges and curriculum planning, and can be highlighted in marketing materials, open house events, and other outreach efforts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4598f1",
   "metadata": {},
   "source": [
    "## Model Limitations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60f0c9",
   "metadata": {},
   "source": [
    "\n",
    "The objective of the research project was to classify the performance of secondary school students in two Portuguese secondary schools. Unfortunately, the study contains a number of inference limitations. First, the dataset is from 2008, therefore it may not reflect the current educational scene. In addition, the study had 395 observations in total, which increases the danger of overfitting and makes it challenging to apply the results to a larger population. In addition, the study was done in Portugal, which may limit its applicability to other nations or areas. \n",
    "\n",
    "Future study could benefit from increasing the dataset to include a larger sample size and a broader range of demographic and socioeconomic variables, potentially encompassing a variety of nations. This would allow for more rigorous studies and findings that may be applied to a wider range of population types. It may aid in the identification of gaps in educational results and provide insight into potential interventions that may be implemented to assist underrepresented populations.  Inclusion of a larger range of indicators, such as cultural influences, mental health, and family environment, could further illuminate the intricate relationship between individual determinants and educational results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44497c",
   "metadata": {},
   "source": [
    "## GitHub and individual contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b3f66",
   "metadata": {},
   "source": [
    "[Github link](https://github.com/Juliaaaachu/LinearModel_Stat303-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505da5c",
   "metadata": {},
   "source": [
    "<html>\n",
    "<style>\n",
    "table, td, th {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "\n",
    "table {\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "th {\n",
    "  text-align: left;\n",
    "}\n",
    "    \n",
    "\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<h2>Individual contribution</h2>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <colgroup>\n",
    "       <col span=\"1\" style=\"width: 15%;\">\n",
    "       <col span=\"1\" style=\"width: 20%;\">\n",
    "       <col span=\"1\" style=\"width: 50%;\">\n",
    "       <col span=\"1\" style=\"width: 15%;\"> \n",
    "    </colgroup>\n",
    "  <tr>\n",
    "    <th>Team member</th>\n",
    "    <th>Contributed aspects</th>\n",
    "    <th>Details</th>\n",
    "    <th>Number of GitHub commits</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Julia Chu</td>\n",
    "    <td> Data Quality Check, Outliers, EDA, Github & Report Management</td>\n",
    "    <td> conducted initial exploratory data analysis along with visualization that provided insights for model development and variable transformation. </td>\n",
    "    <td>17</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Victoria Shi</td>\n",
    "    <td> Data preprocessing; EDA; feature selection; model development </td>\n",
    "    <td> Data preprocessing and preperation; exploratory data analysis for base model development; base model development; feature selection via statistical test; identified relevant variable interactions.</td>\n",
    "    <td>40</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Yiru Zhang</td>\n",
    "    <td>EDA and progress model development</td>\n",
    "    <td>explanatory data analysis for the progress model; visualization and variables selection; progress model development and assessment of prediction.</td>\n",
    "    <td>10</td>    \n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Yuyan Zhang</td>\n",
    "    <td>EDA and nested model development</td>\n",
    "    <td>Exploratory data analysis related to nested model development; nested model development</td>\n",
    "    <td>6</td>    \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1cafe",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb1aad",
   "metadata": {},
   "source": [
    "[1] P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\n",
    "\n",
    "[2] Dipam7. (2021). Student Grade Prediction [Data file]. Kaggle. https://www.kaggle.com/dipam7/student-grade-prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
