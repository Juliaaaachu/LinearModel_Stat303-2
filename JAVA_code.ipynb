{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Project Code\"\n",
    "subtitle: Team Java\n",
    "author: Julia Chu, Victoria Shi, Yiru Zhang, and Yuyan Zhang\n",
    "date: 02/27/2023\n",
    "number-sections: true\n",
    "abstract: _This file contains the code for the project on <student grade>, as part of the STAT303-2 course in Winter 2023_.\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import relevant libraries and packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='white', palette='rainbow', font_scale=1.4)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Import the DecisionTreeRegressor class\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# Import the train_test_split and KFold function\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6734cfc9",
   "metadata": {},
   "source": [
    "## Data quality check / cleaning / preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387af9ae",
   "metadata": {},
   "source": [
    "### Data quality check\n",
    "*By Victoria Shi (mostly) and Julia (unless otherwise indicated)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75dfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/student-mat.csv\")\n",
    "\n",
    "# preview of dataset\n",
    "display(df.head())\n",
    "\n",
    "# basic information about the dataset\n",
    "print(\n",
    "    f\"number of rows (Total Number of Students): {df.shape[0]}\\nnumber of columns: {df.shape[1]}\\ncolumn names\\n{list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421a428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing data --> there are no missing values\n",
    "df.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the datatypes in the dataframe -> there are object types and the numerical (integer) type.\n",
    "display(df.dtypes)\n",
    "\n",
    "num_col = list(df.select_dtypes(include=['int64', 'float64']))\n",
    "cat_col = list(df.select_dtypes(include=['object']))\n",
    "# find the exact categorical columns and integer columns respectively\n",
    "print(f\"numerical columns \\n{num_col}\\n\")\n",
    "print(f\"categorical columns: \\n{cat_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9535749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics (min and max) of numerical columns \n",
    "# only want a rough idea of the general range of values instead of the detailed distributions\n",
    "#so min and max would suffice, instead of the mean or standard deviation\n",
    "\n",
    "df.describe().loc[['min', 'max']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Period 1, 2, and Final Grade Distribution\n",
    "df[['G1', 'G2', 'G3']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca420aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical columns\n",
    "cat_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "columns = []\n",
    "\n",
    "# use loop comprehension to  store the column names and the number of unique values\n",
    "[columns.append([col, df[col].nunique()]) for col in cat_columns]\n",
    "\n",
    "tally_cat_cols = pd.DataFrame(columns, columns=['Column Name', 'Number of Unique Values'])\n",
    "tally_cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507e18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "######---------------Detecting Outliers (Julia) ----------------#########\n",
    "\n",
    "def detect_outlier(_df):\n",
    "    for feature in _df:\n",
    "        column = _df[feature]\n",
    "        mean = np.mean(column)\n",
    "        std = np.std(column)\n",
    "        z_scores = (column - mean) / std\n",
    "        outliers = np.abs(z_scores) > 3\n",
    "\n",
    "        n_outliers = sum(outliers)\n",
    "        if n_outliers > 0:\n",
    "            print(\"{} has {} outliers\".format(column.name, n_outliers))\n",
    "            print(f\"{column[np.where(outliers)[0]].T}\\n\")\n",
    "    return None\n",
    "\n",
    "\n",
    "detect_outlier(df.loc[:, num_col].drop(labels=[\"G1\", \"G2\", \"G3\"], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbebb45a",
   "metadata": {},
   "source": [
    "### Data cleaning and Preparation\n",
    "*By Victoria Shi*\n",
    "\n",
    "From the data quality check, we realized that:\n",
    "\n",
    "1. While there are no missing values or outliers, the data set consists of many categorical variables, so we would need to either encode these variables, dummify them, or convert the data type of these columns from object to numeric.\n",
    "2. For columns with a yes-no response, we implement a binary mapping of 'yes' to 1 and 'no' to 0.\n",
    "3. For categorical columns with two unique values, we converted them to 0/1 binary variables, as the choice of 0/1 would not change the result.\n",
    "4. For columns of categorical variables with more than 2 unique values, we could not map them to numerical values (i.e., mapping distinct values to different integer values) as it would introduce bias due to assuming one value is superior to the other. So instead, we convert them to dummy variables as new predictors.\n",
    "5. While  correlation among predictors is not a major issue, there are still a few columns that have relatively strong correlations: mother education and father education (Medu and Fedu), and grades for period 1, 2, and 3(G1, G2 and G3).\n",
    "\n",
    "The code below implements the above transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f745c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "######---------------Converting yes-no variables to binary variables----------------#########\n",
    "# Create a dictionary for binary mapping\n",
    "binary_mapping = {'yes': 1, 'no': 0}\n",
    "\n",
    "# Find all columns in the data frame that have 'yes' or 'no' as the response\n",
    "yes_no_columns = [col for col in df.columns if df[col].isin(['yes', 'no']).any()]\n",
    "\n",
    "# Apply binary mapping to all columns found in the previous step using a lambda function and pandas' apply method\n",
    "df[yes_no_columns] = df[yes_no_columns].apply(lambda x: x.map(binary_mapping)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d1557",
   "metadata": {},
   "outputs": [],
   "source": [
    "######---------------Transforming predictors with 2 unique values----------------#########\n",
    "\n",
    "# map categorical variables in data to 0 or 1 if the column has two unique values\n",
    "#  map the values in the 'school' column to 0 or 1\n",
    "df['school'] = df['school'].map({'GP': 0, 'MS': 1})\n",
    "\n",
    "# map the values in the 'sex' column to 0 or 1\n",
    "df['sex'] = df['sex'].map({'F': 0, 'M': 1})\n",
    "\n",
    "# map the values in the 'famsize' column to 0 or 1\n",
    "# binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3\n",
    "df['famsize'] = df['famsize'].map({'LE3': 0, 'GT3': 1})\n",
    "\n",
    "# map the values in the 'address' column to 0 or 1\n",
    "# binary: 'U' - urban or 'R' - rural\n",
    "df['address'] = df['address'].map({'R': 0, 'U': 1})\n",
    "\n",
    "# map the values in the 'Pstatus' column to 0 or 1\n",
    "# parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n",
    "df['Pstatus'] = df['Pstatus'].map({'A': 0, 'T': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9232883",
   "metadata": {},
   "outputs": [],
   "source": [
    "######---------------Creating new predictors----------------#########\n",
    "# Create dummy variables for the 'col' column\n",
    "target_cols = ['Mjob', 'Fjob', 'reason', 'guardian']\n",
    "dummies = pd.get_dummies(df.loc[:, target_cols], prefix=['Mjob', 'Fjob', 'reason', 'guardian'])\n",
    "\n",
    "df.drop(columns=target_cols, inplace=True)\n",
    "\n",
    "# Concatenate the original data frame with the dummy variables\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "# combine Dalc and Walc into alc\n",
    "df.loc[:, 'Alc'] = df['Dalc'] + df['Walc']\n",
    "# drop Dalc and Walc\n",
    "df = df.drop(columns=['Dalc', 'Walc'])\n",
    "# combine Fedu and Medu into famEdu\n",
    "famEdu = df['Fedu'] + df['Fedu']\n",
    "df = df.drop(columns=['Fedu', 'Fedu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecc219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_num2 = df.copy()\n",
    "df_model_dev = df.copy()\n",
    "\n",
    "# preview of dataset for training\n",
    "display(df.head())\n",
    "display(df.dtypes)\n",
    "print(\n",
    "    f\"After cleaning and preparation, the dataset has {df.shape[0]} rows and {df.shape[1]} columns.\\nNew columns:\\n {list(df.columns)}\\nAll columns are of uint8 or integer data types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9bb2a1",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "*Julia Chu*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d96b71",
   "metadata": {},
   "source": [
    "We obtained the following insights after part 1 of data exploration:\n",
    "\n",
    "- Motivation to take higher education may play a role in the final grade.\n",
    "- There is a slight difference in grade distribution between genders, but it is not prominent. Statistical significance for gender as a predictor cannot be determined from the graph alone.\n",
    "- The data points are inconsistent, with only three observations for age 20. There appears to be no clear relationship between age, gender, or their interaction, and the final grade.\n",
    "- There is not much difference in scores based on geography (rural or urban).\n",
    "- There may be a high correlation between absences and grades, with grades decreasing as absences increase. The absences are not unlikely to due to health condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa98d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data groupBy()... grouping The Data For higher (if they are willing to take higher education)\n",
    "df.groupby('higher').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Age & Gender Distribution and its effect on grade outcome\n",
    "### Marking Final Grade as Categorical Value & Viewing their Count\n",
    "male_studs = len(df[df['sex'] == 1])\n",
    "female_studs = len(df[df['sex'] == 0])\n",
    "print('Number of male students:', male_studs)\n",
    "print('Number of female students:', female_studs)\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "sns.countplot(data=df, x='age', hue='sex').set_title('Number of students in different age groups', fontsize=12)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e817f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whether there's a gender difference in grade distribution in G1 G2 AND G3\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "ax1 = sns.boxplot(data=df, x='sex', y='G1', ax=axes[0])\n",
    "ax1.set_title('G1')\n",
    "ax2 = sns.boxplot(data=df, x='sex', y='G2', ax=axes[1])\n",
    "ax2.set_title('G2')\n",
    "ax3 = sns.boxplot(data=df, x='sex', y='G3', ax=axes[2])\n",
    "ax3.set_title('G3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd192614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat the variable finalGrade, which is one when a student passes (>=20) and 0 when a student fails (<20)\n",
    "df['finalGrade'] = np.where(df['G3'] >= 12, 'Pass', 'Fail')\n",
    "df['finalGrade'].replace(['Fail', 'Pass'], [0, 1], inplace=True)\n",
    "# Whether there's a gender difference in grade distribution in G1 G2 AND G3\n",
    "fig, axes = plt.subplots(2, 1, figsize=(8, 8))\n",
    "age_data = df.groupby('age')['finalGrade'].agg([('pass_rate', 'mean'), ('nobs', 'count')]).reset_index(drop=False)\n",
    "age_pass = sns.lineplot(x=age_data.age, y=age_data['pass_rate'], ax=axes[0])\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "b = sns.swarmplot(x='age', y='G3', hue='sex', data=df, ax=axes[1])\n",
    "b.axes.set_title('Does age affect final grade?')\n",
    "b.set_xlabel('Age')\n",
    "b.set_ylabel('Final Grade')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e90d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=df, x=\"sex\", y=\"finalGrade\", col=\"age\", kind=\"bar\", height=4, aspect=.6, ax=axes[0])\n",
    "g.set_axis_labels(\"\", \"pass Rate\")\n",
    "g.set_xticklabels([\"Male\", \"Female\"])\n",
    "g.set_titles(\"{col_name} {col_var}\")\n",
    "g.set(ylim=(0, 1))\n",
    "g.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "#Whether geography affects grade outcome \n",
    "add_plt = sns.countplot(data=df, x='address', hue=\"finalGrade\", ax=axes[0])\n",
    "add_plt.axes.set_title('Urban and Rural students')\n",
    "add_plt.set_xlabel('Address')\n",
    "add_plt.set_ylabel('Count')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.125)\n",
    "# Grade distribution by address\n",
    "sns.kdeplot(df.loc[df['address'] == 1, 'G3'], label='Urban', shade=True, ax=axes[1])\n",
    "sns.kdeplot(df.loc[df['address'] == 0, 'G3'], label='Rural', shade=True, ax=axes[1])\n",
    "plt.title('Urban VS Rural Grade Score')\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grade distribution by Absences, and whether absences are correlated with health condition\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 6))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "sns.lineplot(data=df, x='absences', y='G3', ax=axes[0])\n",
    "sns.violinplot(data=df, x='health', y='absences', ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c60e205",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "*Victoria Shi*\n",
    "\n",
    "Insights used for base model development\n",
    "1. Period 1 (`G1`) and Period 2 (`G2`) grades are strongly correlated with each other, as evidenced by the darker shade on the pairwise correlation plot.\n",
    "2. Period 1 (`G1`) and Period 2 (`G2`) grades are highly indicative of the final grade (`G3`), as evidenced by the scatterplot displaying the relationship `between` G1 and `G2`, with data points colored based on `G3`.\n",
    "3. Both the bar plot based on the importance scores of the decision tree and the line plot demonstrate that `absences` and `failures` are crucial for predicting the final grades, with grades decreasing as absences and failures increase.\n",
    "4. `Medu` (mother's education) and `Fedu` (father's education) are strongly correlated, so we combined them into `famEdu` (family education).\n",
    "5. `Dalc` (weekday alcohol consumption) and `Walc` (weekend alcohol consumption) are strongly correlated, so we combined them into `Alc` (alcohol consumption).\n",
    "6. A new correlation plot after removing and combining predictors shows that major dependencies among predictors have been resolved.\n",
    "7. Parents' jobs (`Mjob` and `Fjob`) also affect a student's grades. However, due to the difficulty in generalizing their impact across various categories, we did not focus on this factor during the base model development. Nevertheless, it remains an important aspect to consider in future analyses or more specialized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/student-mat.csv\")\n",
    "# Create a new column 'G3_binary' in the data frame\n",
    "# Assign 0 to all rows where G3 is less than 12\n",
    "data.loc[data.G3 < 12, 'G3_binary'] = 0\n",
    "\n",
    "# Assign 1 to all rows where G3 is greater than or equal to 12\n",
    "data.loc[data.G3 >= 12, 'G3_binary'] = 1\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "# Plot a count plot of the 'G3' column and set the title\n",
    "fig.add_subplot(1, 2, 1)\n",
    "sns.countplot(x='G3', data=data, order=data['G3'].value_counts().index).set_title(\"Final Grade G3 Distribution\")\n",
    "\n",
    "# Plot a count plot of the 'G3_binary' column and set the title\n",
    "fig.add_subplot(1, 2, 2)\n",
    "sns.countplot(x=data.G3_binary, order=data.G3_binary.value_counts().index).set_title(\"Final Grade G3 (Binary) Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6710f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "data = data.drop(columns='G3_binary')\n",
    "# plot the correlation among variables that are ORIGINALLy numerical\n",
    "sns.heatmap(data.corr(), cmap='twilight', annot=True).set_title(\"Pairwise correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75683f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "sns.lineplot(x='G1', y='G3', err_kws={'fc': 'pink'}, data=data).set_title('G1 versus G3')\n",
    "fig.add_subplot(1, 2, 2)\n",
    "sns.lineplot(x='G2', y='G3', err_kws={'fc': 'pink'}, data=data).set_title('G2 versus G3')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='G1', y='G2', hue='G3', palette='rainbow', data=data).set_title(\n",
    "    ' Period 1 (G1) and Period 2 (G2) grades strongly indicate final grade (G3)')\n",
    "plt.hlines(12, 0, 20, color=\"magenta\", linestyle=\"--\")\n",
    "plt.vlines(12, 0, 20, color=\"magenta\", linestyle=\"--\", label=\"Pass/Fail Boundary\")\n",
    "plt.legend(loc=\"lower right\", title=\"G3\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a370851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since G1, G2, and G3 are highly correlated, we remove G2 and G3\n",
    "data = data.drop(columns=[\"G1\", \"G2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a499b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine father education and mother education into alcohol\n",
    "data['famEdu'] = data['Fedu'] + data['Medu']\n",
    "# combine day alc and week into alcohol\n",
    "data['Alc'] = data['Walc'] + data['Dalc']\n",
    "# drop redundant columns\n",
    "data = data.drop(columns=[\"Fedu\", 'Medu', 'Walc', 'Dalc'])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(data.corr(), cmap='twilight', annot=True).set_title(\"Pairwise correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f76122",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24, 24))\n",
    "fig.add_subplot(3, 2, 1)\n",
    "sns.lineplot(x='failures', y='G3', err_kws={'hatch': '///', 'fc': 'pink'}, data=data).set_title(\n",
    "    'Failures (numerical) vs G3')\n",
    "fig.add_subplot(3, 2, 2)\n",
    "sns.lineplot(x='absences', y='G3', err_kws={'hatch': '///', 'fc': 'pink'}, data=data).set_title('Absences (numerical) vs G3')\n",
    "fig.add_subplot(3, 2, 3)\n",
    "sns.lineplot(x='famEdu', y='G3', err_kws={'hatch': '///', 'fc': 'pink'}, data=data).set_title('Family Education (numerical) vs G3')\n",
    "fig.add_subplot(3, 2, 4)\n",
    "data = pd.read_csv(\"data/student-mat.csv\")\n",
    "sns.boxplot(x='schoolsup', y='G3', data=data).set_title('Extra educational support (binary) vs G3')\n",
    "fig.add_subplot(3, 2, 5)\n",
    "sns.boxplot(x='Mjob', y='G3', data=data).set_title('Mother job vs G3')\n",
    "fig.add_subplot(3, 2, 6)\n",
    "sns.boxplot(x='Fjob', y='G3', data=data).set_title('Father job vs G3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42767fa8",
   "metadata": {},
   "source": [
    "### EDA for Progress Model\n",
    "*Yiru Zhang*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf17074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to measure if a student improves from the previous test\n",
    "#create categorical variables G1_G2 and G2_G3, which will be 1 if a students improves (G2/G3 score higher than G1/G2)\n",
    "#and 0 otherwise (no improvement or decrease in grade)\n",
    "df['G1_G2'] = 0\n",
    "df.loc[df.G2 > df.G1, 'G1_G2'] = 1\n",
    "df['G2_G3'] = 0\n",
    "df.loc[df.G3 > df.G2, 'G2_G3'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26341337",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After observing the barplots of all the predictors vs G1 to G2 progress, here are the five graphs that show\n",
    "#a relatively large change in the response variable G1_G2 as the predictor changes value\n",
    "\n",
    "fig = plt.figure(figsize=(24, 24))\n",
    "fig.add_subplot(3, 2, 1)\n",
    "sns.barplot(x='age', y='G1_G2', data=df).set_title('Age vs G1 G2 Progress')\n",
    "#Note: for age 21 and 22, there is only one observation with those ages, so we should not overgeneralize our eda to them\n",
    "#It is over generalization to say that student aged 22 will definitely improve their grades(same thing for age 21)\n",
    "fig.add_subplot(3, 2, 3)\n",
    "sns.barplot(x='reason_other', y='G1_G2', data=df).set_title(\"Reason to choose this school as'other' vs G1 G2 Progress\")\n",
    "fig.add_subplot(3, 2, 4)\n",
    "sns.barplot(x='Mjob_other', y='G1_G2', data=df).set_title(\"Mother's job as 'other' vs G1 G2 Progress\")\n",
    "fig.add_subplot(3, 2, 5)\n",
    "sns.barplot(x='romantic', y='G1_G2', data=df).set_title('Romantic Relationship vs G1 G2 Progress')\n",
    "fig.add_subplot(3, 2, 6)\n",
    "sns.barplot(x='Fjob_services', y='G1_G2', data=df).set_title(\n",
    "    \"Father's job as civil 'services' (e.g. administrative or police) vs G1 G2 Progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3047ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mjob_teacher+famrel+reason_course\n",
    "#After observing the barplots of all the predictors vs G2 to G3 progress, here are the three graphs that show\n",
    "#a relatively large change in the response variable G2_G3 as the predictor changes value\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "fig.add_subplot(2, 2, 1)\n",
    "sns.barplot(x='Mjob_teacher', y='G2_G3', data=df).set_title(\"Mother's job as 'teacher' vs G1 G2 Progress\")\n",
    "fig.add_subplot(2, 2, 2)\n",
    "sns.barplot(x='famrel', y='G2_G3', data=df).set_title(\"Family relationship vs G1 G2 Progress\")\n",
    "fig.add_subplot(2, 2, 3)\n",
    "sns.barplot(x='reason_course', y='G2_G3', data=df).set_title(\n",
    "    \"Reason to choose this school as'course' vs G1 G2 Progress\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06fb00",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "*Yuyan*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have the predictors and response variable ready for Lasso\n",
    "predictors = list(df.columns)[0:43]\n",
    "remove = ['G1', 'G2', 'G3']\n",
    "pred_filter = [x for x in predictors if x not in remove]\n",
    "\n",
    "X = df[pred_filter]\n",
    "y = df.finalGrade\n",
    "\n",
    "# Split the data into training and test sets using the train_test_split function\n",
    "# The test set will be 20% of the total data and the random state is set to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Combining back X and y back to one df\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508ba6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso for variable selection\n",
    "\n",
    "#Scaling X (training data)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "Xstd = scaler.transform(X_train)\n",
    "\n",
    "#Creating a range of values of the tuning parameter\n",
    "alphas = 10 ** np.linspace(10, -2, 100) * 0.1\n",
    "\n",
    "lasso = Lasso(max_iter=10000)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(Xstd, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "#use cross validation to find the optimal value of the tuning parameter - lambda\n",
    "lassocv = LassoCV(alphas=alphas, cv=10, max_iter=100000)\n",
    "lassocv.fit(Xstd, y_train)\n",
    "\n",
    "#Optimal value of the tuning parameter - lamda\n",
    "lassocv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b4d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select the five predictors that have the highest coefficients\n",
    "coeff = pd.concat([pd.Series(X_train.columns), np.abs(pd.Series(lassocv.coef_))], axis=1)\n",
    "coeff.columns = ['predictor', 'coefficient']\n",
    "coeff.sort_values(by='coefficient', ascending=False, inplace=True)\n",
    "predictor_subset = coeff['predictor'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aa8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff4ae62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#building a logistic regression model with selected variables\n",
    "logit_model = smf.logit(formula='finalGrade~' + '+'.join(predictor_subset), data=train).fit(disp=False)\n",
    "logit_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute confusion matrix and prediction accuracy on test/train data\n",
    "def confusion_matrix_data(_data, actual_values, model, cutoff=0.5):\n",
    "    #Predict the values using the Logit model\n",
    "    pred_values = model.predict(_data)\n",
    "    # Specify the bins\n",
    "    bins = np.array([0, cutoff, 1])\n",
    "    #Confusion matrix\n",
    "    cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n",
    "    cm_df = pd.DataFrame(cm)\n",
    "    cm_df.columns = ['Predicted 0', 'Predicted 1']\n",
    "    cm_df = cm_df.rename(index={0: 'Actual 0', 1: 'Actual 1'})\n",
    "    # Calculate the accuracy\n",
    "    accuracy = (cm[0, 0] + cm[1, 1]) / cm.sum()\n",
    "    fnr = (cm[1, 0]) / (cm[1, 0] + cm[1, 1])\n",
    "    precision = (cm[1, 1]) / (cm[0, 1] + cm[1, 1])\n",
    "    fpr = (cm[0, 1]) / (cm[0, 0] + cm[0, 1])\n",
    "    tpr = (cm[1, 1]) / (cm[1, 0] + cm[1, 1])\n",
    "    fpr_roc, tpr_roc, auc_thresholds = roc_curve(actual_values, pred_values)\n",
    "    auc_value = (auc(fpr_roc, tpr_roc))\n",
    "    print(\"Classification accuracy = {:.1%}\".format(accuracy))\n",
    "    print(\"Precision = {:.1%}\".format(precision))\n",
    "    print(\"TPR or Recall = {:.1%}\".format(tpr))\n",
    "    print(\"FNR = {:.1%}\".format(fnr))\n",
    "    print(\"FPR = {:.1%}\".format(fpr))\n",
    "    print(\"ROC-AUC = {:.1%}\".format(auc_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2fa8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Metric for prediction. Threshold of 0.6 was chosen to reduce FPR, while maintaining accuracy. \n",
    "# However, both metrics are not idea (FPR too high, accuracy too low)\n",
    "confusion_matrix_data(train, train.finalGrade, logit_model, cutoff=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22514e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics on test data\n",
    "confusion_matrix_data(test, test.finalGrade, logit_model, cutoff=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866383c",
   "metadata": {},
   "source": [
    "### Base Model Development\n",
    "*Victoria*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa7ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######-----------Shuffling the dataset via train-test split for K-fold------------#########\n",
    "\n",
    "# create a copy of the cleaned and transformed data frame and assign it to data (for base model development)\n",
    "data = df_model_dev.copy()\n",
    "predictor_cols = [col for col in data.columns.tolist() if col not in ['G1', 'G2', 'G3']]\n",
    "\n",
    "X = data[predictor_cols]\n",
    "\n",
    "y = data.G3\n",
    "\n",
    "# Define the KFold cross-validation object with 10 splits, a random state of 1, and shuffling enabled\n",
    "k_fold = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "\n",
    "# Split the data into training and test sets using the train_test_split function\n",
    "# The test set will be 20% of the total data and the random state is set to 1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "print(f\"dimension of training set: {X_train.shape}\\ndimension of testing set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28d0272",
   "metadata": {},
   "source": [
    "#### Important Predictors (Overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd932d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a DecisionTreeRegressor object and set the random state to 1 for reproducibility\n",
    "model_tree = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_tree.fit(X_train, y_train)\n",
    "\n",
    "# Create a pandas Series object to store the feature importance\n",
    "importance = pd.Series(model_tree.feature_importances_, index=X.columns)\n",
    "\n",
    "# Plot the 10 largest feature importance using a horizontal bar plot\n",
    "importance.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0951d6",
   "metadata": {},
   "source": [
    "#### Important Categorical Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c05fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X.drop(columns='Alc')\n",
    "\n",
    "# Select only categorical columns from the dataframe X_cat\n",
    "X_cat = X_cat.loc[:, [col for col in X_cat.columns if col not in num_col]]\n",
    "\n",
    "# Apply SelectKBest with chi-square test to select the best 5 features\n",
    "k_best = SelectKBest(score_func=chi2, k=5).fit(X_cat, y)\n",
    "\n",
    "# Create a pandas series to store the scores of each feature\n",
    "df_score = pd.Series(data=k_best.scores_, index=X_cat.columns)\n",
    "\n",
    "# Sort the scores in descending order\n",
    "df_score.sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a88a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate X_train and y_train so that predictors and response are in the same dataset\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# concatenate X_test and y_test so that predictors and response are in the same dataset\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to test prediction accuracy\n",
    "def acc(_lm, _Xtest, _ytest):\n",
    "    y_pred = _lm.predict(_Xtest)\n",
    "    df = pd.concat([y_pred, _ytest], axis=1)\n",
    "    test_res = pd.DataFrame(df, columns=[\"predicted\", \"actual\"], dtype=\"float64\")\n",
    "    test_res[y_pred < 12], test_res[y_pred >= 12] = 0, 1\n",
    "    test_res[y_pred < 6], test_res[y_pred >= 12] = 0, 1\n",
    "    y_test[y_test < 12], y_test[y_test >= 12] = 0, 1\n",
    "    return np.mean(test_res.predicted == test_res.actual)\n",
    "\n",
    "\n",
    "# function to format and print result\n",
    "def display_res(lm_formula):\n",
    "    lm = smf.ols(formula=lm_formula, data=train).fit()\n",
    "    p_vals = lm.pvalues\n",
    "    acc_test = acc(lm, test, y_test)\n",
    "    acc_full = acc(lm, X, y)\n",
    "    print(f\"------ {lm_formula} ------\\nP-values \\n{p_vals}\\nAccuracy on test set: {acc_test}\\tfull dataset: {acc_full}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm1 = 'G3 ~ absences'\n",
    "display_res(lm1)\n",
    "\n",
    "lm2 = 'G3 ~ failures'\n",
    "display_res(lm2)\n",
    "\n",
    "lm3 = 'G3 ~ absences + failures'\n",
    "display_res(lm2)\n",
    "\n",
    "lm4 = 'G3 ~ absences * failures'\n",
    "display_res(lm4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75cce0",
   "metadata": {},
   "source": [
    "### Progress Model Development\n",
    "*Yiru Zhang*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have the predictors and response variable ready for Lasso\n",
    "predictors = list(df.columns)[0:43]\n",
    "remove = ['G1', 'G2', 'G3']\n",
    "pred_filter = [x for x in predictors if x not in remove]\n",
    "X = df[pred_filter]\n",
    "y1 = df.G1_G2\n",
    "y2 = df.G2_G3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c077f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso for G1_G2\n",
    "#Scaling X\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "Xstd = scaler.transform(X)\n",
    "\n",
    "#Creating a range of values of the tuning parameter\n",
    "alphas = 10 ** np.linspace(10, -2, 100) * 0.1\n",
    "\n",
    "lasso1 = Lasso(max_iter=10000)\n",
    "coefs1 = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso1.set_params(alpha=a)\n",
    "    lasso1.fit(Xstd, y1)\n",
    "    coefs1.append(lasso1.coef_)\n",
    "\n",
    "#use cross validation to find the optimal value of the tuning parameter - lambda\n",
    "lassocv1 = LassoCV(alphas=alphas, cv=10, max_iter=100000)\n",
    "lassocv1.fit(Xstd, y1)\n",
    "\n",
    "#Optimal value of the tuning parameter - lamda\n",
    "lassocv1.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f88866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the five predictors that have the highest coefficients\n",
    "coeff1 = pd.concat([pd.Series(X.columns), np.abs(pd.Series(lasso1.coef_))], axis=1)\n",
    "coeff1.columns = ['predictor', 'coefficient']\n",
    "coeff1.sort_values(by='coefficient', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0202e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model built for G1_G2, using the variables from Lasso\n",
    "progress_model_1_1 = smf.logit(formula='G1_G2~age+reason_other+Mjob_other+romantic+famrel', data=df).fit(disp=False)\n",
    "progress_model_1_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93a45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data(df, df.G1_G2, progress_model_1_1, cutoff=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model built for G1_G2, using the variables from EDA\n",
    "progress_model_1_2 = smf.logit(formula='G1_G2~age+reason_other+Mjob_other+romantic+Fjob_services', data=df).fit(disp=False)\n",
    "progress_model_1_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d39db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data(df, df.G1_G2, progress_model_1_2, cutoff=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97302517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso for G2_G3\n",
    "#Scaling X\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "Xstd = scaler.transform(X)\n",
    "\n",
    "#Creating a range of values of the tuning parameter\n",
    "alphas = 10 ** np.linspace(10, -2, 100) * 0.1\n",
    "\n",
    "lasso2 = Lasso(max_iter=10000)\n",
    "coefs2 = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso2.set_params(alpha=a)\n",
    "    lasso2.fit(Xstd, y2)\n",
    "    coefs2.append(lasso2.coef_)\n",
    "\n",
    "#use cross validation to find the optimal value of the tuning parameter - lambda\n",
    "lassocv2 = LassoCV(alphas=alphas, cv=10, max_iter=100000)\n",
    "lassocv2.fit(Xstd, y2)\n",
    "\n",
    "#Optimal value of the tuning parameter - lamda\n",
    "lassocv2.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e518f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff2 = pd.concat([pd.Series(X.columns), np.abs(pd.Series(lasso2.coef_))], axis=1)\n",
    "coeff2.columns = ['predictor', 'coefficient']\n",
    "coeff2.sort_values(by='coefficient', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcf98e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model built for G2_G3, using the variables from Lasso\n",
    "progress_model_2_1 = smf.logit(formula='G2_G3~+Mjob_teacher+reason_course+famrel+address+goout', data=df).fit(disp=False)\n",
    "progress_model_2_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2655de",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_data(df, df.G1_G2, progress_model_2_1, cutoff=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f35a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model built for G2_G3, using the variables from EDA\n",
    "progress_model_2_2 = smf.logit(formula='G2_G3~+Mjob_teacher+reason_course+famrel', data=df).fit(disp=False)\n",
    "progress_model_2_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "424a20ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy = 56.2%\n",
      "Precision = 29.4%\n",
      "TPR or Recall = 25.0%\n",
      "FNR = 75.0%\n",
      "FPR = 28.8%\n",
      "ROC-AUC = 46.1%\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_data(df, df.G1_G2, progress_model_2_2, cutoff=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444e611",
   "metadata": {},
   "source": [
    "### Working final model\n",
    "#### Base model\n",
    "*Victoria*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "id": "206a9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Model\n",
    "base_model_final = smf.ols(formula='G3 ~ absences * failures', data = df).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d938d2e",
   "metadata": {},
   "source": [
    "#### Progress Model\n",
    "*Alice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "3220e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model built for G1_G2\n",
    "progress_model_G1_G2_final = smf.logit(formula='G1_G2~age+reason_other+Mjob_other+romantic+famrel', data=df).fit(disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "65cbdbfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model built for G2_G3\n",
    "progress_model_G2_G3_final = smf.logit(formula='G2_G3~+Mjob_teacher+reason_course+famrel', data=df).fit(disp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c0883",
   "metadata": {},
   "source": [
    "*Insights derived from the logistic regression model - Yuyan*\n",
    "\n",
    "1. The number of failures a student has experienced, whether they receive school support, mother's job (particularly if they are a teacher), mother's level of education, and sex are all significant when it comes to classifying whether a student passes or fails (low p-value, model itself has high LLR p-value).\n",
    "\n",
    "2. Based on this model, with each unit increase in failure, the odds of them passing this class are multiplied by np.exp(-1.3128) or 0.269. In other words, the odds of passing the current class decreases by 80% for each additional class they have failed in the past. While there are likely factors that play into both past class failures and current class performance, this suggests that this cycling of failing is hard to break.\n",
    "\n",
    "*Insights derived from the base model - Victoria*\n",
    "\n",
    "3. Absences and failures are important quantitative predictors in determining a student's grades. As such, it is essential to consider these factors when developing early warning systems aimed at ensuring the academic success of students who may be susceptible to these challenges. By closely monitoring students' attendance and performance, educators can intervene proactively and provide tailored support to help students overcome the obstacles that absences and failures may present. Implementing such a system will not only help to identify students at risk but also enable timely interventions that can significantly enhance their chances of academic success and overall well-being.\n",
    "\n",
    "4. School support and parents' jobs have emerged as important quantitative predictors of students' grades in our analysis. These factors significantly influence a student's academic performance, highlighting the need to consider them when designing early warning systems. By identifying students who may be susceptible to the challenges associated with limited school support or specific parental occupations, targeted interventions can be developed to ensure their academic success.\n",
    "\n",
    "*Insights derived from the progress model - Yiru*\n",
    "\n",
    "5. Mother's job plays an important part for both the improvement from G1 to G2 and G2 to G3. This variable Mjob consists the categories 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other', and being a teacher and other jobs have positive relationship with the increase in grade for students. I believe probably health care and civil service jobs require a lot of time and effort, and mothers do not have enough time to accompany their kids and help with their learning. Also, housewives might not have enough knowledge to help their kids study.\n",
    "\n",
    "6. Also, the reason why a students chooses this school is related to their progress. The variable consists of values: close to 'home', school 'reputation', 'course' preference or 'other'. From the two models, reason_other has positive coefficient in the model for G1_G2 and reason_course has negative coefficient in the model for G2_G3. I think one major category that contains in 'other' might be achieving personal growth and learn knowledge, and this could stimulate student's interest in studying, and thus improvement in scores. In contrast, if the student only thinks the courses are interesting, they will not care much about their grades, so the coefficient in front of reason_course will be negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
